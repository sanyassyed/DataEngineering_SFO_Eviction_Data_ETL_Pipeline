{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fef7dcc",
   "metadata": {},
   "source": [
    "## Downloading data as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9290d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config, AutoConfig\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec929fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig(search_path='.env') # <-- .env file located next to manage.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "873e922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the json by supplying the api token in the header\n",
    "def get_json(endpoint, headers):\n",
    "    \"\"\"download the json by supplying the api token in the header\"\"\"\n",
    "    headers['Accept'] = 'application/json' # csv?\n",
    "    # pull_date = (datetime.now() - timedelta(days=180)).strftime(\"%Y-%m-%dT%H:%M:%S\") # year, month, day, hour, minute, seconds, microseconds\n",
    "    combined = []\n",
    "    offset, counter = 0, 1\n",
    "    error = False\n",
    "    params = f\"\"\"$query=SELECT:*,* ORDER BY :id LIMIT 2000\"\"\"\n",
    "    # response has two parts .json() and .headers https://www.w3schools.com/python/ref_requests_response.asp\n",
    "    response = requests.get(endpoint, headers=headers, params=params)\n",
    "    captured = response.json()\n",
    "    combined.extend(captured)\n",
    "    print('get_json complete')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ed2747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_json complete\n"
     ]
    }
   ],
   "source": [
    "# Sodu API Credentials\n",
    "API_TOKEN = config(\"API_TOKEN\")\n",
    "API_KEY_ID = config(\"API_KEY_ID\")\n",
    "API_KEY_SECRET = config(\"API_KEY_SECRET\")\n",
    "\n",
    "source_path_json = '/home/sanyashireen/sf_eviction/data_eviction/2023/3/26/api_raw_eviction_2023-03-26.json'\n",
    "data_dir = Path('/home/sanyashireen/sf_eviction/data_eviction/2023/3/26')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SODA_url = 'https://data.sfgov.org/resource/5cei-gny5'\n",
    "SODA_headers = {\n",
    "    'keyId': API_KEY_ID,\n",
    "    'keySecret': API_KEY_SECRET\n",
    "}\n",
    "content = get_json(SODA_url, SODA_headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd32f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path_json = '/home/sanyashireen/sf_eviction/data_eviction/2023/3/26/api_raw_eviction_2023-03-26.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fefd3ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f9d4dd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(content[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf78565a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jsdon.dumps() returns a json str\n",
    "type(json.dumps(content, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45029f8e",
   "metadata": {},
   "source": [
    "## TESTING: json.dump() instead of json.dumps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc2f1193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing writing the list of dictionaries (response.json()) returned by the API to a file locally as json file\n",
    "# using json.dump\n",
    "import json\n",
    "out_file = open(source_path_json,\"w\", encoding='utf8')\n",
    "json.dump(content, out_file, indent=4)\n",
    "out_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "278329f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and testing script to clean and transform csv data using pyspark\n",
    "# spark related packages\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed65b098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "source_path_json = '/home/sanyashireen/sf_eviction/data_eviction/2023/3/26/raw_eviction_2023-03-26.json'\n",
    "df = spark.read.option(\"multiline\",\"true\").json(str(source_path_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240c9f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "177578"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80b5e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- :@computed_region_26cr_cadq: string (nullable = true)\n",
      " |-- :@computed_region_6ezc_tdp2: string (nullable = true)\n",
      " |-- :@computed_region_6pnf_4xz7: string (nullable = true)\n",
      " |-- :@computed_region_6qbp_sg9q: string (nullable = true)\n",
      " |-- :@computed_region_9jxd_iqea: string (nullable = true)\n",
      " |-- :@computed_region_ajp5_b2md: string (nullable = true)\n",
      " |-- :@computed_region_bh8s_q3mv: string (nullable = true)\n",
      " |-- :@computed_region_fyvs_ahh9: string (nullable = true)\n",
      " |-- :@computed_region_h4ep_8xdi: string (nullable = true)\n",
      " |-- :@computed_region_jwn9_ihcz: string (nullable = true)\n",
      " |-- :@computed_region_p5aj_wyqh: string (nullable = true)\n",
      " |-- :@computed_region_pigm_ib2e: string (nullable = true)\n",
      " |-- :@computed_region_qgnn_b9vv: string (nullable = true)\n",
      " |-- :@computed_region_rxqg_mtj9: string (nullable = true)\n",
      " |-- :@computed_region_yftq_j783: string (nullable = true)\n",
      " |-- :created_at: string (nullable = true)\n",
      " |-- :id: string (nullable = true)\n",
      " |-- :updated_at: string (nullable = true)\n",
      " |-- :version: string (nullable = true)\n",
      " |-- access_denial: boolean (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- breach: boolean (nullable = true)\n",
      " |-- capital_improvement: boolean (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- client_location: struct (nullable = true)\n",
      " |    |-- human_address: string (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |-- condo_conversion: boolean (nullable = true)\n",
      " |-- constraints_date: string (nullable = true)\n",
      " |-- demolition: boolean (nullable = true)\n",
      " |-- development: boolean (nullable = true)\n",
      " |-- ellis_act_withdrawal: boolean (nullable = true)\n",
      " |-- eviction_id: string (nullable = true)\n",
      " |-- failure_to_sign_renewal: boolean (nullable = true)\n",
      " |-- file_date: string (nullable = true)\n",
      " |-- good_samaritan_ends: boolean (nullable = true)\n",
      " |-- illegal_use: boolean (nullable = true)\n",
      " |-- late_payments: boolean (nullable = true)\n",
      " |-- lead_remediation: boolean (nullable = true)\n",
      " |-- neighborhood: string (nullable = true)\n",
      " |-- non_payment: boolean (nullable = true)\n",
      " |-- nuisance: boolean (nullable = true)\n",
      " |-- other_cause: boolean (nullable = true)\n",
      " |-- owner_move_in: boolean (nullable = true)\n",
      " |-- roommate_same_unit: boolean (nullable = true)\n",
      " |-- shape: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- substantial_rehab: boolean (nullable = true)\n",
      " |-- supervisor_district: string (nullable = true)\n",
      " |-- unapproved_subtenant: boolean (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5484a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/27 14:34:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(:@computed_region_26cr_cadq='2', :@computed_region_6ezc_tdp2=None, :@computed_region_6pnf_4xz7='2', :@computed_region_6qbp_sg9q='53', :@computed_region_9jxd_iqea=None, :@computed_region_ajp5_b2md='20', :@computed_region_bh8s_q3mv='28859', :@computed_region_fyvs_ahh9='19', :@computed_region_h4ep_8xdi=None, :@computed_region_jwn9_ihcz='53', :@computed_region_p5aj_wyqh='4', :@computed_region_pigm_ib2e=None, :@computed_region_qgnn_b9vv='3', :@computed_region_rxqg_mtj9='7', :@computed_region_yftq_j783='2', :created_at='2023-01-23T23:46:16.858Z', :id='row-ufzj_22gk~drek', :updated_at='2023-01-23T23:46:33.925Z', :version='rv-wryr~aezj~xte7', access_denial=False, address='2500 Block Of Folsom  Street', breach=False, capital_improvement=False, city='San Francisco', client_location=Row(human_address='{\"address\": \"\", \"city\": \"\", \"state\": \"\", \"zip\": \"\"}', latitude='37.75649855484188', longitude='-122.41446453496935'), condo_conversion=False, constraints_date=None, demolition=False, development=False, ellis_act_withdrawal=False, eviction_id='M221207', failure_to_sign_renewal=False, file_date='2022-12-23T00:00:00.000', good_samaritan_ends=False, illegal_use=False, late_payments=False, lead_remediation=False, neighborhood='Mission', non_payment=True, nuisance=False, other_cause=False, owner_move_in=False, roommate_same_unit=False, shape=Row(coordinates=[-122.41447, 37.7565], type='Point'), state='CA', substantial_rehab=False, supervisor_district='9', unapproved_subtenant=False, zip='94110'),\n",
       " Row(:@computed_region_26cr_cadq='9', :@computed_region_6ezc_tdp2=None, :@computed_region_6pnf_4xz7='1', :@computed_region_6qbp_sg9q='55', :@computed_region_9jxd_iqea='14', :@computed_region_ajp5_b2md='26', :@computed_region_bh8s_q3mv='28856', :@computed_region_fyvs_ahh9='29', :@computed_region_h4ep_8xdi=None, :@computed_region_jwn9_ihcz='55', :@computed_region_p5aj_wyqh='3', :@computed_region_pigm_ib2e=None, :@computed_region_qgnn_b9vv='2', :@computed_region_rxqg_mtj9='8', :@computed_region_yftq_j783='14', :created_at='2023-01-23T23:46:16.858Z', :id='row-k4ar_jqfk~qf9p', :updated_at='2023-01-23T23:46:33.925Z', :version='rv-jrf5.m8q7.bb2j', access_denial=False, address='2000 Block Of 03rd  Street', breach=False, capital_improvement=False, city='San Francisco', client_location=Row(human_address='{\"address\": \"\", \"city\": \"\", \"state\": \"\", \"zip\": \"\"}', latitude='37.7636861675663', longitude='-122.38873210572237'), condo_conversion=False, constraints_date=None, demolition=False, development=False, ellis_act_withdrawal=False, eviction_id='M221222', failure_to_sign_renewal=False, file_date='2022-12-23T00:00:00.000', good_samaritan_ends=False, illegal_use=False, late_payments=False, lead_remediation=False, neighborhood='Potrero Hill', non_payment=True, nuisance=False, other_cause=False, owner_move_in=False, roommate_same_unit=False, shape=Row(coordinates=[-122.38873, 37.763687], type='Point'), state='CA', substantial_rehab=False, supervisor_district='10', unapproved_subtenant=False, zip='94107'),\n",
       " Row(:@computed_region_26cr_cadq='10', :@computed_region_6ezc_tdp2=None, :@computed_region_6pnf_4xz7='2', :@computed_region_6qbp_sg9q='20', :@computed_region_9jxd_iqea=None, :@computed_region_ajp5_b2md='36', :@computed_region_bh8s_q3mv='28858', :@computed_region_fyvs_ahh9='36', :@computed_region_h4ep_8xdi=None, :@computed_region_jwn9_ihcz='20', :@computed_region_p5aj_wyqh='1', :@computed_region_pigm_ib2e=None, :@computed_region_qgnn_b9vv='6', :@computed_region_rxqg_mtj9='9', :@computed_region_yftq_j783='13', :created_at='2023-01-23T23:46:16.858Z', :id='row-g7er.wz7c~98am', :updated_at='2023-01-23T23:46:33.925Z', :version='rv-kub4-qfcm-vv5s', access_denial=False, address='800 Block Of Geary  Street', breach=False, capital_improvement=False, city='San Francisco', client_location=Row(human_address='{\"address\": \"\", \"city\": \"\", \"state\": \"\", \"zip\": \"\"}', latitude='37.78620563104688', longitude='-122.41726861276132'), condo_conversion=False, constraints_date=None, demolition=False, development=False, ellis_act_withdrawal=False, eviction_id='M220927', failure_to_sign_renewal=False, file_date='2022-12-22T00:00:00.000', good_samaritan_ends=False, illegal_use=False, late_payments=False, lead_remediation=False, neighborhood='Tenderloin', non_payment=False, nuisance=True, other_cause=False, owner_move_in=False, roommate_same_unit=False, shape=Row(coordinates=[-122.41727, 37.786205], type='Point'), state='CA', substantial_rehab=False, supervisor_district='3', unapproved_subtenant=False, zip='94109')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ca95ebdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+--------------------+------------------+--------------------+-----------------+-------------+--------------------+------+-------------------+-------------+--------------------+----------------+----------------+----------+-----------+--------------------+-----------+-----------------------+--------------------+-------------------+-----------+-------------+----------------+--------------------+-----------+--------+-----------+-------------+------------------+--------------------+-----+-----------------+-------------------+--------------------+-----+\n",
      "|:@computed_region_26cr_cadq|:@computed_region_6ezc_tdp2|:@computed_region_6pnf_4xz7|:@computed_region_6qbp_sg9q|:@computed_region_9jxd_iqea|:@computed_region_ajp5_b2md|:@computed_region_bh8s_q3mv|:@computed_region_fyvs_ahh9|:@computed_region_h4ep_8xdi|:@computed_region_jwn9_ihcz|:@computed_region_p5aj_wyqh|:@computed_region_pigm_ib2e|:@computed_region_qgnn_b9vv|:@computed_region_rxqg_mtj9|:@computed_region_yftq_j783|         :created_at|               :id|         :updated_at|         :version|access_denial|             address|breach|capital_improvement|         city|     client_location|condo_conversion|constraints_date|demolition|development|ellis_act_withdrawal|eviction_id|failure_to_sign_renewal|           file_date|good_samaritan_ends|illegal_use|late_payments|lead_remediation|        neighborhood|non_payment|nuisance|other_cause|owner_move_in|roommate_same_unit|               shape|state|substantial_rehab|supervisor_district|unapproved_subtenant|  zip|\n",
      "+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+--------------------+------------------+--------------------+-----------------+-------------+--------------------+------+-------------------+-------------+--------------------+----------------+----------------+----------+-----------+--------------------+-----------+-----------------------+--------------------+-------------------+-----------+-------------+----------------+--------------------+-----------+--------+-----------+-------------+------------------+--------------------+-----+-----------------+-------------------+--------------------+-----+\n",
      "|                          2|                       null|                          2|                         53|                       null|                         20|                      28859|                         19|                       null|                         53|                          4|                       null|                          3|                          7|                          2|2023-01-23T23:46:...|row-ufzj_22gk~drek|2023-01-23T23:46:...|rv-wryr~aezj~xte7|        false|2500 Block Of Fol...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221207|                  false|2022-12-23T00:00:...|              false|      false|        false|           false|             Mission|       true|   false|      false|        false|             false|{[-122.41447, 37....|   CA|            false|                  9|               false|94110|\n",
      "|                          9|                       null|                          1|                         55|                         14|                         26|                      28856|                         29|                       null|                         55|                          3|                       null|                          2|                          8|                         14|2023-01-23T23:46:...|row-k4ar_jqfk~qf9p|2023-01-23T23:46:...|rv-jrf5.m8q7.bb2j|        false|2000 Block Of 03r...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221222|                  false|2022-12-23T00:00:...|              false|      false|        false|           false|        Potrero Hill|       true|   false|      false|        false|             false|{[-122.38873, 37....|   CA|            false|                 10|               false|94107|\n",
      "|                         10|                       null|                          2|                         20|                       null|                         36|                      28858|                         36|                       null|                         20|                          1|                       null|                          6|                          9|                         13|2023-01-23T23:46:...|row-g7er.wz7c~98am|2023-01-23T23:46:...|rv-kub4-qfcm-vv5s|        false|800 Block Of Gear...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M220927|                  false|2022-12-22T00:00:...|              false|      false|        false|           false|          Tenderloin|      false|    true|      false|        false|             false|{[-122.41727, 37....|   CA|            false|                  3|               false|94109|\n",
      "|                          9|                       null|                          1|                         55|                         14|                         26|                      28856|                         29|                       null|                         55|                          3|                       null|                          2|                          8|                         14|2023-01-23T23:46:...|row-jvdz~8ne5_agq5|2023-01-23T23:46:...|rv-t6vv~sqbg.fcxv|        false|2000 Block Of 03r...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221154|                  false|2022-12-22T00:00:...|              false|      false|        false|           false|        Potrero Hill|       true|   false|      false|        false|             false|{[-122.38873, 37....|   CA|            false|                 10|               false|94107|\n",
      "|                          7|                       null|                          1|                         39|                       null|                         35|                      29491|                         35|                       null|                         39|                          8|                       null|                         10|                          3|                          1|2023-01-23T23:46:...|row-er7z~7fk9~55mx|2023-01-23T23:46:...|rv-8f6e~9dpn.id6z|        false|2000 Block Of 44t...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221155|                  false|2022-12-22T00:00:...|              false|      false|        false|           false|     Sunset/Parkside|      false|    true|      false|        false|             false|{[-122.50294, 37....|   CA|            false|                  4|               false|94116|\n",
      "|                          2|                       null|                          2|                         91|                       null|                         25|                        309|                         28|                       null|                         91|                          3|                       null|                          2|                          7|                         10|2023-01-23T23:46:...|row-gr32-umab-fxqu|2023-01-23T23:46:...|rv-d76w-xwwa~2fpx|        false|100 Block Of Gira...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|                true|    M222201|                  false|2022-12-22T00:00:...|              false|      false|        false|           false|             Portola|      false|   false|      false|        false|             false|{[-122.40584, 37....|   CA|            false|                  9|               false|94134|\n",
      "|                          8|                       null|                          1|                         44|                       null|                         41|                      29491|                         40|                       null|                         46|                          8|                       null|                         10|                          4|                          1|2023-01-23T23:46:...|row-5jy6_s62c~m2g4|2023-01-23T23:46:...|rv-pbei~25pq-sufa|         true|500 Block Of Tara...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221156|                  false|2022-12-22T00:00:...|              false|      false|        false|           false|  West of Twin Peaks|      false|   false|      false|        false|             false|{[-122.47192, 37....|   CA|            false|                  7|               false|94116|\n",
      "|                         10|                          1|                          2|                         20|                          6|                         36|                      28852|                         36|                          1|                         20|                         10|                         18|                          5|                          9|                         14|2023-01-23T23:46:...|row-vzi9_7svc~98jq|2023-01-23T23:46:...|rv-md38~jqdr~y94e|        false|0 Block Of Turk  ...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221025|                  false|2022-12-21T00:00:...|              false|      false|        false|           false|          Tenderloin|      false|    true|      false|        false|             false|{[-122.40984, 37....|   CA|            false|                  5|               false|94102|\n",
      "|                         10|                          1|                          2|                         20|                          6|                         36|                      28852|                         36|                          1|                         20|                         10|                         18|                          5|                          9|                          7|2023-01-23T23:46:...|row-hin8.uhzw.rh2d|2023-01-23T23:46:...|rv-r87s.ubap-ypha|        false|400 Block Of Eddy...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221024|                  false|2022-12-21T00:00:...|              false|      false|        false|           false|          Tenderloin|      false|    true|      false|        false|             false|{[-122.41506, 37....|   CA|            false|                  5|               false|94109|\n",
      "|                         10|                          1|                          2|                         20|                          6|                         36|                      28852|                         36|                          1|                         20|                         10|                         18|                          5|                          9|                          7|2023-01-23T23:46:...|row-c8et.y7q5~zf5a|2023-01-23T23:46:...|rv-hhns.jcpx_zh4i|        false|400 Block Of Eddy...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221023|                  false|2022-12-21T00:00:...|              false|      false|        false|           false|          Tenderloin|      false|    true|      false|        false|             false|{[-122.41506, 37....|   CA|            false|                  5|               false|94109|\n",
      "|                          1|                       null|                          2|                         66|                       null|                         28|                      28861|                         25|                       null|                         58|                          7|                       null|                          9|                          6|                          9|2023-01-23T23:46:...|row-nmbc.3tuz-iiid|2023-01-23T23:46:...|rv-9uxf-a7fa-pxdj|        false|5500 Block Of Mis...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M220918|                  false|2022-12-20T00:00:...|              false|      false|        false|           false|       Outer Mission|      false|    true|      false|        false|             false|{[-122.44535, 37....|   CA|            false|                 11|               false|94112|\n",
      "|                         10|                          1|                          2|                         20|                          6|                         36|                      28852|                         36|                          1|                         20|                         10|                         18|                          5|                          9|                         14|2023-01-23T23:46:...|row-4tbe-8w98.9pv7|2023-01-23T23:46:...|rv-bmaz-epwh~xmyb|        false|0 Block Of Turk  ...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M220917|                  false|2022-12-20T00:00:...|              false|      false|        false|           false|          Tenderloin|      false|    true|      false|        false|             false|{[-122.40984, 37....|   CA|            false|                  5|               false|94102|\n",
      "|                         10|                       null|                          1|                         30|                          3|                          8|                      28855|                          6|                       null|                         30|                          2|                       null|                          1|                          9|                          6|2023-01-23T23:46:...|row-q9z3.hksj~buci|2023-01-23T23:46:...|rv-u7ss~juiu-q22s|        false|400 Block Of Harr...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M220817|                  false|2022-12-19T00:00:...|              false|      false|         true|           false|Financial Distric...|      false|   false|      false|        false|             false|{[-122.392494, 37...|   CA|            false|                  6|               false|94105|\n",
      "|                          2|                       null|                          2|                         53|                       null|                         20|                      28859|                         19|                       null|                         53|                          4|                         15|                          3|                          7|                          2|2023-01-23T23:46:...|row-xzpa_5gqk_tkaa|2023-01-23T23:46:...|rv-54mz-m79f~ka2u|        false|2300 Block Of Mis...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M220807|                  false|2022-12-15T00:00:...|              false|      false|        false|           false|             Mission|      false|    true|      false|        false|             false|{[-122.41913, 37....|   CA|            false|                  9|               false|94110|\n",
      "|                         10|                       null|                          1|                         30|                          3|                          8|                      28855|                          6|                       null|                         30|                          2|                       null|                          1|                          9|                          6|2023-01-23T23:46:...|row-dbnj~pgmw~9psn|2023-01-23T23:46:...|rv-avj4~ti4q.umve|        false|400 Block Of Harr...|  true|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M220730|                  false|2022-12-15T00:00:...|              false|      false|        false|           false|Financial Distric...|      false|   false|      false|        false|             false|{[-122.392494, 37...|   CA|            false|                  6|               false|94105|\n",
      "|                          4|                       null|                          1|                          8|                       null|                         29|                         55|                         26|                       null|                          8|                          6|                       null|                          8|                          2|                         11|2023-01-23T23:46:...|row-8z8h.ufhb-gfi2|2023-01-23T23:46:...|rv-btrk~v8zb~gte7|        false|4400 Block Of Cab...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M220700|                  false|2022-12-14T00:00:...|              false|      false|        false|           false|      Outer Richmond|      false|    true|      false|        false|             false|{[-122.50629, 37....|   CA|            false|                  1|               false|94121|\n",
      "|                         10|                          1|                          2|                         20|                          6|                         36|                      28852|                         36|                          1|                         20|                          9|                         18|                          5|                          9|                          7|2023-01-23T23:46:...|row-qaa9.jdir~57ak|2023-01-23T23:46:...|rv-83i6~54kg~sg9k|        false|500 Block Of Lark...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221307|                  false|2022-12-13T00:00:...|              false|      false|        false|           false|          Tenderloin|       true|   false|      false|        false|             false|{[-122.417435, 37...|   CA|            false|                  5|               false|94102|\n",
      "|                         10|                       null|                          1|                         34|                       null|                          4|                        310|                         20|                       null|                         34|                          3|                       null|                          1|                          9|                         14|2023-01-23T23:46:...|row-dvbm~dwx5.pkw2|2023-01-23T23:46:...|rv-b4ew-r2v4.5p8r|        false|400 Block Of Chin...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221306|                  false|2022-12-13T00:00:...|              false|      false|        false|           false|         Mission Bay|      false|    true|      false|        false|             false|{[-122.388245, 37...|   CA|            false|                  6|               false|94158|\n",
      "|                         10|                       null|                          2|                         32|                       null|                         20|                      28853|                         19|                       null|                         32|                          2|                       null|                          1|                          9|                          8|2023-01-23T23:46:...|row-gi9x-ueyq.jh22|2023-01-23T23:46:...|rv-b337.3dm3~j89f|        false|1600 Block Of Mar...| false|              false|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M221308|                  false|2022-12-13T00:00:...|              false|      false|        false|           false|             Mission|      false|    true|      false|        false|             false|{[-122.42223, 37....|   CA|            false|                  6|               false|94103|\n",
      "|                          7|                       null|                          1|                         39|                       null|                         35|                         56|                         35|                       null|                         39|                          8|                       null|                         10|                          3|                          1|2023-01-23T23:46:...|row-b8gi_67ir_wxsf|2023-01-23T23:46:...|rv-87zc-yxgg-ugxv|        false|1200 Block Of 22n...| false|               true|San Francisco|{{\"address\": \"\", ...|           false|            null|     false|      false|               false|    M220845|                  false|2022-12-12T00:00:...|              false|      false|        false|           false|     Sunset/Parkside|      false|   false|      false|        false|             false|{[-122.4804, 37.7...|   CA|            false|                  4|               false|94122|\n",
      "+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+---------------------------+--------------------+------------------+--------------------+-----------------+-------------+--------------------+------+-------------------+-------------+--------------------+----------------+----------------+----------+-----------+--------------------+-----------+-----------------------+--------------------+-------------------+-----------+-------------+----------------+--------------------+-----------+--------+-----------+-------------+------------------+--------------------+-----+-----------------+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50bf9449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':@computed_region_26cr_cadq',\n",
       " ':@computed_region_6ezc_tdp2',\n",
       " ':@computed_region_6pnf_4xz7',\n",
       " ':@computed_region_6qbp_sg9q',\n",
       " ':@computed_region_9jxd_iqea',\n",
       " ':@computed_region_ajp5_b2md',\n",
       " ':@computed_region_bh8s_q3mv',\n",
       " ':@computed_region_fyvs_ahh9',\n",
       " ':@computed_region_h4ep_8xdi',\n",
       " ':@computed_region_jwn9_ihcz',\n",
       " ':@computed_region_p5aj_wyqh',\n",
       " ':@computed_region_pigm_ib2e',\n",
       " ':@computed_region_qgnn_b9vv',\n",
       " ':@computed_region_rxqg_mtj9',\n",
       " ':@computed_region_yftq_j783',\n",
       " ':created_at',\n",
       " ':id',\n",
       " ':updated_at',\n",
       " ':version',\n",
       " 'access_denial',\n",
       " 'address',\n",
       " 'breach',\n",
       " 'capital_improvement',\n",
       " 'city',\n",
       " 'client_location',\n",
       " 'condo_conversion',\n",
       " 'constraints_date',\n",
       " 'demolition',\n",
       " 'development',\n",
       " 'ellis_act_withdrawal',\n",
       " 'eviction_id',\n",
       " 'failure_to_sign_renewal',\n",
       " 'file_date',\n",
       " 'good_samaritan_ends',\n",
       " 'illegal_use',\n",
       " 'late_payments',\n",
       " 'lead_remediation',\n",
       " 'neighborhood',\n",
       " 'non_payment',\n",
       " 'nuisance',\n",
       " 'other_cause',\n",
       " 'owner_move_in',\n",
       " 'roommate_same_unit',\n",
       " 'shape',\n",
       " 'state',\n",
       " 'substantial_rehab',\n",
       " 'supervisor_district',\n",
       " 'unapproved_subtenant',\n",
       " 'zip']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55a399e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40e4acf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pyspark df is a list of records where each record is of type <class 'pyspark.sql.types.Row'> \n",
      "\n",
      " Row(:@computed_region_26cr_cadq='2', :@computed_region_6ezc_tdp2=None, :@computed_region_6pnf_4xz7='2', :@computed_region_6qbp_sg9q='53', :@computed_region_9jxd_iqea=None, :@computed_region_ajp5_b2md='20', :@computed_region_bh8s_q3mv='28859', :@computed_region_fyvs_ahh9='19', :@computed_region_h4ep_8xdi=None, :@computed_region_jwn9_ihcz='53', :@computed_region_p5aj_wyqh='4', :@computed_region_pigm_ib2e=None, :@computed_region_qgnn_b9vv='3', :@computed_region_rxqg_mtj9='7', :@computed_region_yftq_j783='2', :created_at='2023-01-23T23:46:16.858Z', :id='row-ufzj_22gk~drek', :updated_at='2023-01-23T23:46:33.925Z', :version='rv-wryr~aezj~xte7', access_denial=False, address='2500 Block Of Folsom  Street', breach=False, capital_improvement=False, city='San Francisco', client_location=Row(human_address='{\"address\": \"\", \"city\": \"\", \"state\": \"\", \"zip\": \"\"}', latitude='37.75649855484188', longitude='-122.41446453496935'), condo_conversion=False, constraints_date=None, demolition=False, development=False, ellis_act_withdrawal=False, eviction_id='M221207', failure_to_sign_renewal=False, file_date='2022-12-23T00:00:00.000', good_samaritan_ends=False, illegal_use=False, late_payments=False, lead_remediation=False, neighborhood='Mission', non_payment=True, nuisance=False, other_cause=False, owner_move_in=False, roommate_same_unit=False, shape=Row(coordinates=[-122.41447, 37.7565], type='Point'), state='CA', substantial_rehab=False, supervisor_district='9', unapproved_subtenant=False, zip='94110')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'The pyspark df is a list of records where each record is of type {type(df.head(1)[0])} \\n\\n {df.head(1)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "982a37f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pyspark df is a list of records where each record is of type <class 'pyspark.sql.types.Row'> \n",
      "\n",
      " Row(:@computed_region_26cr_cadq='2', :@computed_region_6ezc_tdp2=None, :@computed_region_6pnf_4xz7='2', :@computed_region_6qbp_sg9q='53', :@computed_region_9jxd_iqea=None, :@computed_region_ajp5_b2md='20', :@computed_region_bh8s_q3mv='28859', :@computed_region_fyvs_ahh9='19', :@computed_region_h4ep_8xdi=None, :@computed_region_jwn9_ihcz='53', :@computed_region_p5aj_wyqh='4', :@computed_region_pigm_ib2e=None, :@computed_region_qgnn_b9vv='3', :@computed_region_rxqg_mtj9='7', :@computed_region_yftq_j783='2', :created_at='2023-01-23T23:46:16.858Z', :id='row-ufzj_22gk~drek', :updated_at='2023-01-23T23:46:33.925Z', :version='rv-wryr~aezj~xte7', access_denial=False, address='2500 Block Of Folsom  Street', breach=False, capital_improvement=False, city='San Francisco', client_location=Row(human_address='{\"address\": \"\", \"city\": \"\", \"state\": \"\", \"zip\": \"\"}', latitude='37.75649855484188', longitude='-122.41446453496935'), condo_conversion=False, constraints_date=None, demolition=False, development=False, ellis_act_withdrawal=False, eviction_id='M221207', failure_to_sign_renewal=False, file_date='2022-12-23T00:00:00.000', good_samaritan_ends=False, illegal_use=False, late_payments=False, lead_remediation=False, neighborhood='Mission', non_payment=True, nuisance=False, other_cause=False, owner_move_in=False, roommate_same_unit=False, shape=Row(coordinates=[-122.41447, 37.7565], type='Point'), state='CA', substantial_rehab=False, supervisor_district='9', unapproved_subtenant=False, zip='94110')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Result of json.dumps - to compare\n",
    "# print(f'The pyspark df is a list of records where each record is of type {type(df.head(1)[0])} \\n\\n {df.head(1)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9f50a",
   "metadata": {},
   "source": [
    "## TESTING: If Pyspark can read the json data directly from json string\n",
    "* Not working\n",
    "* Ref here for later\n",
    "    - https://ch-nabarun.medium.com/read-json-using-pyspark-f792bda95741\n",
    "    - the above blog uses SparkContext (to convert json to RDD) and SparkSession (to convert RDD to DF)\n",
    "    - Cant figure out how to \n",
    "    - https://stackoverflow.com/questions/39818368/convert-lines-of-json-in-rdd-to-dataframe-in-apache-spark\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4d6e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = json.dumps(content, indent=4)\n",
    "#json_string = json.dumps(content, indent=4, ensure_ascii=False).encode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb512dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and testing script to clean and transform csv data using pyspark\n",
    "# spark related packages\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eb984cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69b936e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing if pyspark can directly read the json data\n",
    "\n",
    "# Step 1 convert the json data to rdd using sparkContext\n",
    "# convert into RDD\n",
    "rdd = spark.sparkContext.parallelize(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb385dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97b381ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/27 15:27:18 WARN TaskSetManager: Stage 2 contains a task of very large size (1993 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/03/27 15:27:18 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 8)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/27 15:27:18 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 8) (de-zoomcamp.us-central1-c.c.blissful-flames-375219.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "23/03/27 15:27:18 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 8) (de-zoomcamp.us-central1-c.c.blissful-flames-375219.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1880\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1882\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1883\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1885\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   1886\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 8) (de-zoomcamp.us-central1-c.c.blissful-flames-375219.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2249b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/27 15:26:14 WARN TaskSetManager: Stage 1 contains a task of very large size (1993 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/03/27 15:26:14 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 4)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "23/03/27 15:26:14 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 4) (de-zoomcamp.us-central1-c.c.blissful-flames-375219.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\n",
      "23/03/27 15:26:14 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n",
      "23/03/27 15:26:14 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 5) (de-zoomcamp.us-central1-c.c.blissful-flames-375219.internal executor driver): TaskKilled (Stage cancelled)\n",
      "23/03/27 15:26:14 WARN TaskSetManager: Lost task 3.0 in stage 1.0 (TID 7) (de-zoomcamp.us-central1-c.c.blissful-flames-375219.internal executor driver): TaskKilled (Stage cancelled)\n",
      "23/03/27 15:26:14 WARN TaskSetManager: Lost task 2.0 in stage 1.0 (TID 6) (de-zoomcamp.us-central1-c.c.blissful-flames-375219.internal executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o248.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 4) (de-zoomcamp.us-central1-c.c.blissful-flames-375219.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:116)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$6(DataFrameReader.scala:415)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:415)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:390)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:376)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create a Dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultiline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/readwriter.py:299\u001b[0m, in \u001b[0;36mDataFrameReader.json\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m keyed\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mBytesToString())\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjrdd\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath can be only string, list or RDD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o248.json.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 4) (de-zoomcamp.us-central1-c.c.blissful-flames-375219.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2333)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:116)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:110)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$json$6(DataFrameReader.scala:415)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:415)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:390)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:376)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/sanyashireen/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.9 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat scala.collection.AbstractIterator.isEmpty(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:103)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# create a Dataframe\n",
    "df = spark.read.option(\"multiline\",\"true\").json(rdd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-myenv-kernel",
   "language": "python",
   "name": "conda-myenv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
